# 似然估计函数, softmax

在深度学习中，似然估计函数（Likelihood Function）是**模型参数的函数**，它衡量的是：

> **在给定模型参数下，观测到的数据出现的“可能性”有多大。**

---

### ✅ 一句话理解：
>
> **似然函数告诉我们：这套参数能不能“解释”我们观测到的数据。**

---

### 📌 举个例子：

假设你有一个神经网络，用来分类猫和狗。  
你有一堆图片（数据），每张图有标签（猫 or 狗）。

- 你用一个参数集合 θ（网络权重）来建模。
- 对于某张图片 x，模型输出一个概率分布 p(y|x; θ)，比如 p(猫|x) = 0.9。

**似然函数就是：**
> 把所有图片的预测概率乘起来，看这组参数 θ 能让“真实标签”出现的概率有多大。

---

### 📊 数学形式：

如果你有数据集 {(x₁, y₁), ..., (xₙ, yₙ)}，似然函数是：

\[
L(\theta) = \prod_{i=1}^n p(y_i \mid x_i; \theta)
\]

通常取对数，变成**对数似然函数**：

\[
\log L(\theta) = \sum_{i=1}^n \log p(y_i \mid x_i; \theta)
\]

---

### 🎯 目的：
>
> **最大化似然函数**（即最大似然估计，MLE），就是找到一组参数 θ，使得模型对观测数据的“解释力”最强。

---

### ✅ 总结一句话：
>
> **似然函数是深度学习中衡量“模型参数好不好”的标尺，训练目标就是让它最大化。**

---

如果你熟悉交叉熵损失函数，那它和负对数似然是等价的：

\[
\text{CrossEntropyLoss} = -\log L(\theta)
\]

所以，**最小化交叉熵 = 最大化似然**。

## softmax 函数是干啥的？为啥叫 softmax ?

一句话先给你答案：  
**Softmax 把任意一组实数“压缩”成一个合法的概率分布（和为 1，全为正），从而让神经网络的最后一层可以输出“似然概率”。**  
之所以叫 *soft-max*，是因为它把“取最大值”这一硬操作（hard max）换成了“按指数加权平均”的软操作（soft max）。

---

### 1. Softmax 做了什么？

输入：一个长度为 K 的向量 **z** = (z₁, …, z_K)  
输出：一个同样长度的向量 **p** = (p₁, …, p_K)，满足

\[
p_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}, \quad \text{保证 } \sum_i p_i = 1,\; p_i>0
\]

- 把 logit（未归一化的得分）变成概率。  
- 哪个 z_i 越大，对应的 p_i 越接近 1，但不会真的等于 1（除非其他项是 −∞）。  
- 因此它“软化”了 max 的赢者通吃。

---

### 2. 为什么叫 softmax？

- **hard max**：只保留最大的那个元素为 1，其余变 0（one-hot）。  
- **soft max**：用指数放大差距，再用归一化得到“软”的概率分布。  
→ 既突出了最大值，又保留了所有类别的信息，便于梯度传播。

---

### 3. 如何用 softmax 计算输出单元的“似然概率”？

在分类任务里，网络的最后一层通常给出 K 个 logit z₁…z_K。  
把 softmax 套上去，就得到：

\[
p(y=k \mid x; \theta) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
\]

- 这里的 p(y=k|x;θ) 就是“给定输入 x 和参数 θ，模型认为类别 k 的似然概率”。  
- 训练时用最大似然（等价于最小化交叉熵），不断调整参数 θ，使真实类别对应的 p 尽可能接近 1。

---

### 4. 小结口诀

Softmax 做三件事：  
1. 归一化——保证输出是合法的概率；  
2. 放大差异——指数让大更大，小更小；  
3. 可导——处处光滑，反向传播无压力。