# [深度学习 误差函数 的 交叉熵代价函数是什么?][def]

交叉熵代价函数（Cross-Entropy Cost Function）是深度学习中分类任务最常用的误差函数之一，
尤其适用于**多分类（softmax输出）**与**二分类（sigmoid输出）**问题。
它衡量了模型预测的概率分布与真实标签的概率分布之间的差异，本质上是信息论中**交叉熵**概念的延伸。

---

### 1. 核心定义

对于**多分类问题**（类别数为 \( K \)），交叉熵代价函数的数学表达式为：

\[
L = -\sum_{i=1}^K y_i \log(p_i)
\]

其中：
- \( y_i \) 是真实标签的**one-hot编码**（正确类别为1，其余为0）。
- \( p_i \) 是模型预测的属于类别 \( i \) 的概率（通过softmax输出）。
- 对数通常取自然对数（底数为 \( e \)）。

对于**二分类问题**（sigmoid输出），公式简化为：

\[
L = -[y \log(p) + (1-y) \log(1-p)]
\]

其中 \( y \in \{0,1\} \) 是真实标签，\( p \) 是模型预测为正类的概率。

---

### 2. 关键特性

- **梯度消失缓解**：与均方误差（MSE）不同，交叉熵的梯度在预测错误时更大，能加速模型收敛。
- **概率解释**：直接优化预测概率与真实分布的“距离”，而非单纯预测值与标签的数值差异。
- **凸性**：对于逻辑回归（单层sigmoid），交叉熵代价函数是凸函数，保证全局最优。

---

### 3. 实际应用示例

假设一个三分类问题，真实标签为类别2（one-hot编码为 \([0,1,0]\)），模型softmax输出概率为 \([0.2, 0.7, 0.1]\)，则交叉熵为：

\[
L = -(0 \cdot \log 0.2 + 1 \cdot \log 0.7 + 0 \cdot \log 0.1) = -\log 0.7 \approx 0.357
\]

---

### 4. 与最大似然估计的关系

交叉熵最小化等价于**最大化似然函数**的对数。在分类问题中，这表示模型试图找到使观测数据出现概率最大的参数。

---

### 5. 代码实现（PyTorch示例）

```python
import torch
import torch.nn as nn

# 多分类交叉熵
criterion = nn.CrossEntropyLoss()  # 内部自动应用softmax
outputs = torch.tensor([[0.2, 0.7, 0.1]])  # 模型原始输出（未归一化）
labels = torch.tensor([1])  # 类别2的索引为1
loss = criterion(outputs, labels)
print(loss.item())  # 输出约0.357
```

---

总结：交叉熵代价函数通过衡量预测概率与真实标签的差异，为分类模型提供了高效的优化目标，是深度学习分类任务的基石。

[def]: https://www.kimi.com/share/d2soovcbcdro73icmabg